{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Basic study before Show, Attend, Tell\n",
    "\n",
    "This notebook summarize what I have studied to implement this repository. If you are newbi to image like me, I recommend to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. RNN\n",
    "I study contents from [this blog](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9).\n",
    "\n",
    "API list\n",
    "- RNNCell in pytorch [here](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html#torch.nn.RNNCell). RNNCell requires both of hidden and input to have exact same batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 5\n",
    "input_dim = 10\n",
    "hidden_dim = 15\n",
    "\n",
    "x = torch.rand([batch_size, input_dim])\n",
    "init_h = torch.rand([batch_size, hidden_dim])\n",
    "f = nn.RNNCell(input_dim, hidden_dim)\n",
    "next_h = f(x, init_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. LSTM (from [this wikidocs in korean](https://wikidocs.net/22888))\n",
    "\n",
    "In summary, there are 3 main in/output for LSTM.\n",
    "\n",
    "- Cell State (Long-term memory)\n",
    "- Hidden State = Output (Short-term memory)\n",
    "- input\n",
    "\n",
    "To compute Cell State, you should compute\n",
    "\n",
    "- `forgot_gate(x, hidden_state) * cell state + input_gate(x, hidden_state)`\n",
    "- `forgot_gate(x, hidden_state)` = `sigmoid(W_hf@hidden_state` + `W_xf@x)`\n",
    "- `input_gate(x, hidden_state)` = `sigmoid(W_hi@hidden_state + W_xi@x) + tanh(W_hg@hidden_state + W_xg@x)`\n",
    "\n",
    "To compute Output (hidden_state), you should compute\n",
    "\n",
    "- Compute Cell State, `C_t`\n",
    "- `tanh(C_t) * output_gate(x, hidden_state)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 5\n",
    "input_dim = 10\n",
    "hidden_dim = 15\n",
    "\n",
    "x = torch.rand([batch_size, input_dim])\n",
    "init_h = torch.rand([batch_size, hidden_dim])\n",
    "init_c = torch.rand([batch_size, hidden_dim])\n",
    "f = nn.LSTMCell(input_dim, hidden_dim)\n",
    "next_h, next_c = f(x, (init_h, init_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. pytorch padded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([1, 4, 2, 5, 3]), batch_sizes=tensor([2, 2, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch = [torch.tensor([1,2,3]), torch.tensor([4,5])]\n",
    "packed_batch = nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "packed_sequence_batch = nn.utils.rnn.pack_padded_sequence(packed_batch, [3, 2], batch_first=True)\n",
    "print(packed_sequence_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}